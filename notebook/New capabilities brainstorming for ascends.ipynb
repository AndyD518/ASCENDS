{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import ascends as asc\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1fd9303e8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LMP\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_y\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0masc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_load_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols_to_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/slz/git_projects/ascends-toolkit/ascends.py\u001b[0m in \u001b[0;36mdata_load_shuffle\u001b[0;34m(csv_file, input_col, cols_to_remove, target_col, random_state, delimiter, map_all)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "csv_file = \"data/creep_more_features.csv\"\n",
    "input_col = None\n",
    "cols_to_remove = [\"id\", \"NAME\", \"RuptureTime\"]\n",
    "target_col = \"LMP\"\n",
    "\n",
    "data_df, x_train, y_train, header_x, header_y =\\\n",
    "asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_analysis(col_name, only_minmax = False):\n",
    "    \n",
    "    array_values = data_df[col_name].values\n",
    "    final_dict = {}\n",
    "    final_dict['col_mean'] = np.mean(array_values)\n",
    "    final_dict['col_stdev'] = np.std(array_values)\n",
    "    final_dict['col_min'] = np.min(array_values)\n",
    "    final_dict['col_max'] = np.max(array_values)\n",
    "\n",
    "    if only_minmax !=True:\n",
    "        fs_dict, final_report = asc.correlation_analysis_all(data_df, target_col)\n",
    "        for key in final_report.loc[col_name].keys():\n",
    "            final_dict[key] = final_report.loc[col_name,key]\n",
    "            try:\n",
    "                final_dict[key+\"_rank\"] = int(final_report.rank(axis=0, ascending=False).loc[col_name, key])\n",
    "            except:\n",
    "                final_dict[key+\"_rank\"] = -1\n",
    "            \n",
    "    return final_dict\n",
    "\n",
    "def show_histogram(col_name):\n",
    "    ax = data_df[col_name].plot.hist(bins=10, alpha=0.5)\n",
    "\n",
    "def show_scatter(col_name):\n",
    "    ax = data_df.plot.scatter(x=col_name,y=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'xFE'\n",
    "\n",
    "final_dict = feature_analysis(col_name)\n",
    "print(final_dict)\n",
    "show_histogram(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scatter(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_data_by_value(data_df, col_name, value):\n",
    "    data_df_by_value = data_df.loc[data_df[col_name]==value]\n",
    "    return data_df_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conditional_over_threshold(col_list, constraint_list, num_of_data_point_threshold=10, threshold=0.8, criteria='PCC_SQRT'):\n",
    "    print(\"* Analyzing features .. \")\n",
    "    for constraint_key in constraint_list:   \n",
    "        print(\"\\n\", constraint_key, \"being processed .. \")\n",
    "        distinct_vals = sorted(list(set(data_df[constraint_key].values)))\n",
    "        print(\"number of distinct values = \", len(distinct_vals))\n",
    "        if len(distinct_vals)>30:\n",
    "            for val in distinct_vals:\n",
    "                data_df_by_value = select_data_by_value(data_df, constraint_key, val)\n",
    "                for col in col_list:\n",
    "                    num_of_data_points = data_df_by_value[col].count()\n",
    "                    if num_of_data_points>num_of_data_point_threshold and col!=constraint_key:\n",
    "                        fs_dict, final_report = asc.correlation_analysis_all(data_df_by_value, target_col)\n",
    "                        criteria_val = final_report[criteria].loc[col]\n",
    "\n",
    "                        if criteria_val>threshold:\n",
    "                            print(\"[ \"+col+\" vs. \"+target_col+\" ] when\", constraint_key.strip(), '=', val, \", '\"+criteria+\"' = \", final_report[criteria].loc[col],'# of data points=', num_of_data_points)\n",
    "                            ax = data_df_by_value.plot.scatter(x=col,y=target_col)\n",
    "                            plt.show()\n",
    "    print(\"- Done - \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_analysis_col = [item for item in header_x if item not in ['T2','T1','Stress','dT']]\n",
    "#conditional_over_threshold(col_list = feature_analysis_col, constraint_list = header_x, \\\n",
    "#                           threshold = 0.9, num_of_data_point_threshold = 10, criteria='PCC_SQRT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_feature_test(number_of_features = 10, test_limit = 5000, top_k = 10, model_type=\"LR\"):\n",
    "    \n",
    "    test_cnt = 0\n",
    "    test_result = {}\n",
    "\n",
    "    while True:\n",
    "        if test_cnt%500==0:\n",
    "            print(test_cnt, \"processed ..\")\n",
    "        test_cnt+=1\n",
    "        input_col = random.sample(list(header_x), number_of_features)\n",
    "        data_df_sample, x_train_sample, y_train_sample, header_x_sample, header_y_sample = asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)\n",
    "        model_parameters = asc.default_model_parameters()\n",
    "        model = asc.define_model_regression(model_type, model_parameters, x_header_size = x_train_sample.shape[1])\n",
    "        predictions, actual_values = asc.train_and_predict(model, x_train_sample, y_train_sample, scaler_option='StandardScaler', num_of_folds=5)\n",
    "        MAE, R2 = asc.evaluate(predictions, actual_values)\n",
    "        for col in input_col:\n",
    "            try:\n",
    "                test_result[col].append(R2) \n",
    "            except:\n",
    "                test_result[col] = [R2]\n",
    "        if test_cnt==test_limit:\n",
    "            break\n",
    "            \n",
    "    test_result_avg = {}\n",
    "    for key in test_result.keys():\n",
    "        test_result_avg[key] = np.mean(test_result[key])\n",
    "\n",
    "    sorted_x = sorted(test_result_avg.items(), key=operator.itemgetter(1), reverse= True)\n",
    "    return sorted_x[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar_features(header_x, threshold=0.9):\n",
    "    result_dict = {}\n",
    "    touched = {}\n",
    "    \n",
    "    data_df_corr = data_df.corr()\n",
    "    for index, row in data_df_corr.iterrows():\n",
    "        for i in header_x:\n",
    "            corr_val = row[i]\n",
    "            try:\n",
    "                touched[i]\n",
    "            except:\n",
    "                touched[i] = False\n",
    "            if touched[i] == False and index>i and corr_val>threshold and index!=i:\n",
    "                try:\n",
    "                    result_dict[index].append(i)\n",
    "                except:\n",
    "                    result_dict[index] = [i]  \n",
    "                touched[i] = True\n",
    "                \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = find_similar_features(header_x, threshold=0.95)\n",
    "pprint(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(x_train)\n",
    "\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_train_pca_df = pd.DataFrame(data = x_train_pca\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "x_train_pca_df_target = pd.concat([x_train_pca_df, data_df[[target_col]]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "ax.scatter(x_train_pca_df_target[['principal component 1']]\n",
    "               , x_train_pca_df_target[['principal component 2']], \n",
    "           c=x_train_pca_df_target[['LMP']].values, edgecolor='none', alpha=0.9, \n",
    "           cmap=plt.cm.get_cmap('coolwarm', 10))\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "x_train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_n = 5\n",
    "pca = PCA(n_components=dim_n)\n",
    "pca.fit_transform(x_train)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "model_parameters = asc.default_model_parameters()\n",
    "model = asc.define_model_regression('LR', model_parameters, x_header_size = x_train_pca.shape[1])\n",
    "predictions, actual_values = asc.train_and_predict(model, x_train_pca, y_train, scaler_option='StandardScaler', num_of_folds=5)\n",
    "MAE, R2 = asc.evaluate(predictions, actual_values)\n",
    "print(\"# of features = \",dim_n, MAE, R2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to simulate experiments\n",
    "# generate input and test\n",
    "\n",
    "scaler_option = 'StandardScaler'\n",
    "data_df, x_train, y_train, header_x, header_y =\\\n",
    "asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)\n",
    "model_parameters = asc.default_model_parameters()\n",
    "model = asc.define_model_regression(\"LR\", model_parameters, x_header_size = x_train.shape[1])\n",
    "x_train_, scale = asc.rescale_x(scaler_option, x_train)\n",
    "y_train_ = y_train.reshape(y_train.shape[0],)\n",
    "model.fit(x_train_, y_train_)\n",
    "\n",
    "find_key = 'xNB' # 변경시킬 값\n",
    "key_idx = 0\n",
    "for i in range(0, len(header_x)):\n",
    "    if header_x[i]==find_key:\n",
    "        key_idx = i\n",
    "        break\n",
    "\n",
    "final_dict = feature_analysis(find_key)\n",
    "num_of_test = 10\n",
    "train_x_to_test = []\n",
    "idx_to_select = 0 # 어떤 기준점에서 값을 바꿔갈 것인가, 고정할 값들을 결정\n",
    "\n",
    "selected_x_train = x_train[idx_to_select]\n",
    "for item in np.linspace(final_dict['col_min'],final_dict['col_max'],num_of_test):\n",
    "    new_x_train = selected_x_train.copy()\n",
    "    new_x_train[key_idx] = item\n",
    "    train_x_to_test.append(new_x_train)\n",
    "    \n",
    "train_x_to_test_ = scale.transform(train_x_to_test)\n",
    "result = model.predict(train_x_to_test_)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel(find_key, fontsize = 15)\n",
    "ax.set_ylabel(target_col, fontsize = 15)\n",
    "ax.set_title(target_col+' when changing '+find_key, fontsize = 20)\n",
    "ax.scatter(np.linspace(final_dict['col_min'],final_dict['col_max'],num_of_test)\n",
    "               , result)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to simulate experiments\n",
    "# generate input and test\n",
    "\n",
    "# 모든 데이터 포인트를 고려\n",
    "\n",
    "scaler_option = 'StandardScaler'\n",
    "data_df, x_train, y_train, header_x, header_y =\\\n",
    "asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)\n",
    "model_parameters = asc.default_model_parameters()\n",
    "model = asc.define_model_regression(\"RF\", model_parameters, x_header_size = x_train.shape[1])\n",
    "x_train_, scale = asc.rescale_x(scaler_option, x_train)\n",
    "y_train_ = y_train.reshape(y_train.shape[0],)\n",
    "model.fit(x_train_, y_train_)\n",
    "\n",
    "find_key = 'xNB' # 변경시킬 값\n",
    "key_idx = 0\n",
    "for i in range(0, len(header_x)):\n",
    "    if header_x[i]==find_key:\n",
    "        key_idx = i\n",
    "        break\n",
    "\n",
    "final_dict = feature_analysis(find_key)\n",
    "num_of_test = 10\n",
    "train_x_to_test = []\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for item in np.linspace(final_dict['col_min'],final_dict['col_max'],num_of_test):\n",
    "    new_x_trains = []\n",
    "    for x_train_item in x_train:\n",
    "        new_x_train = x_train_item.copy()\n",
    "        new_x_train[key_idx] = item\n",
    "        new_x_trains.append(new_x_train)\n",
    "        xs.append(item)\n",
    "    \n",
    "    new_x_trains_ = scale.transform(new_x_trains)\n",
    "    result = model.predict(new_x_trains_)\n",
    "    for i in result:\n",
    "        ys.append(i)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel(find_key, fontsize = 15)\n",
    "ax.set_ylabel(target_col, fontsize = 15)\n",
    "ax.set_title(target_col+' when changing '+find_key, fontsize = 20)\n",
    "ax.scatter(xs,ys)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to simulate experiments\n",
    "# generate input and test\n",
    "\n",
    "# 모든 데이터 포인트를 고려\n",
    "\n",
    "for key in header_x:\n",
    "    scaler_option = 'StandardScaler'\n",
    "    data_df, x_train, y_train, header_x, header_y =\\\n",
    "    asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)\n",
    "    model_parameters = asc.default_model_parameters()\n",
    "    model = asc.define_model_regression(\"RF\", model_parameters, x_header_size = x_train.shape[1])\n",
    "    x_train_, scale = asc.rescale_x(scaler_option, x_train)\n",
    "    y_train_ = y_train.reshape(y_train.shape[0],)\n",
    "    model.fit(x_train_, y_train_)\n",
    "\n",
    "    find_key = key # 변경시킬 값\n",
    "    key_idx = 0\n",
    "    for i in range(0, len(header_x)):\n",
    "        if header_x[i]==find_key:\n",
    "            key_idx = i\n",
    "            break\n",
    "\n",
    "    final_dict = feature_analysis(find_key)\n",
    "    num_of_test = 10\n",
    "    train_x_to_test = []\n",
    "\n",
    "    xs = []\n",
    "\n",
    "\n",
    "    ys = []\n",
    "    num_of_inputs = 50\n",
    "\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    means = []\n",
    "    generated_input = []\n",
    "    for i in range(0, len(header_x)):\n",
    "        final_dict = feature_analysis(header_x[i], only_minmax = True)\n",
    "        mins.append(final_dict['col_min'])\n",
    "        maxs.append(final_dict['col_max'])\n",
    "        means.append(final_dict['col_mean'])\n",
    "\n",
    "    x_tests = []\n",
    "    x_interest_col = []\n",
    "\n",
    "    for k in range(0, num_of_inputs):\n",
    "        generated_input = []\n",
    "        for i in range(0, len(header_x)):\n",
    "            rand_val = random.uniform(mins[i],maxs[i])\n",
    "            \n",
    "            if i==key_idx:\n",
    "                x_interest_col.append(rand_val)\n",
    "            else:\n",
    "                rand_val = means[i]\n",
    "            generated_input.append(rand_val)\n",
    "            \n",
    "        x_tests.append(generated_input)\n",
    "\n",
    "    x_tests_ = scale.transform(x_tests)\n",
    "    result = model.predict(x_tests_)\n",
    "\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel(find_key, fontsize = 15)\n",
    "    ax.set_ylabel(target_col, fontsize = 15)\n",
    "    ax.set_title(target_col+' when changing (others mean value)'+find_key, fontsize = 20)\n",
    "    ax.scatter(x_interest_col, result)\n",
    "    ax.grid()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
