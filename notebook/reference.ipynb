{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ascends as asc\n",
    "import keras\n",
    "import ast\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Regression API reference\n",
    "\n",
    "# * NOTE: Ascends-toolkit was developed to be used via command-line interface or web-based interface; however, if needed,\n",
    "# users may use ascends-toolkit's API. The following shows an example of performing a classification task using \n",
    "# the core ascends-toolkit APIs. \n",
    "\n",
    "# You need to download the example data file from https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/data/iris.csv\n",
    "# and save it under data folder\n",
    "\n",
    "csv_file = 'data/iris.csv'\n",
    "cols_to_remove = []\n",
    "target_col = 'Name'\n",
    "input_col = None\n",
    "\n",
    "# Classifier will need a mapping between categorical values to numerical values\n",
    "mapping = {'Name': {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}}\n",
    "\n",
    "# Load data from csv file\n",
    "# A standard csv file can be loaded and shuffled as follows\n",
    "\n",
    "data_df, x_train, y_train, header_x, header_y = asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col, map_all = mapping, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is loaded\n",
    "data_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a default model parameters\n",
    "model_parameters = asc.default_model_parameters_classifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'RF'\n",
    "scaler_option = 'StandardScaler' # scaler option can be 'False','StandardScaler','Normalizer','MinMaxScaler','RobustScaler'\n",
    "num_of_folds = 5\n",
    "model = asc.define_model_classifier(model_type, model_parameters, x_header_size = x_train.shape[1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        50\n",
      "         1.0       0.94      0.92      0.93        50\n",
      "         2.0       0.92      0.94      0.93        50\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       150\n",
      "   macro avg       0.95      0.95      0.95       150\n",
      "weighted avg       0.95      0.95      0.95       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn's classification report can be used to understand the accuracy of the trained model\n",
    "predictions, actual_values = asc.train_and_predict(model, x_train, y_train, scaler_option=scaler_option, num_of_folds=num_of_folds)\n",
    "accuracy = asc.evaluate_classifier(predictions, actual_values)\n",
    "print(\"\")\n",
    "print(\"* Classification Report\")\n",
    "print(classification_report(actual_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training initiated ..\n",
      "* Training done.\n",
      "* Trained model saved to file: model.pkl\n"
     ]
    }
   ],
   "source": [
    "asc.train_and_save_classifier(model, \"model.pkl\", model_type\n",
    "                            , input_cols=header_x, target_col=header_y\n",
    "                            , x_train=x_train, y_train=y_train, scaler_option=scaler_option, path_to_save = '.', accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled x_to_predict =  [[-1.62768837 -1.51337555 -1.45500383  3.94594202]]\n",
      "* Your model thinks that it's a  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# You can load the saved model by using pickle package\n",
    "model_dict = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Let's assume that we have a input as follows\n",
    "x_to_predict = [[4.5, 2.4, 1.2, 4.2]]\n",
    "\n",
    "# You can scale the data using the loaded scaler\n",
    "scaler = model_dict['fitted_scaler_x']\n",
    "x_to_predict = scaler.transform(x_to_predict)\n",
    "print(\"Scaled x_to_predict = \", x_to_predict)\n",
    "\n",
    "# Making prediction can be done as follows\n",
    "predicted_y = model.predict(x_to_predict)\n",
    "\n",
    "# Original prediction value will not be a class name, so you need to find out the class name by doing:\n",
    "for key in mapping['Name'].keys():\n",
    "    if mapping['Name'][key]==predicted_y[0]:\n",
    "        print(\"* Your model thinks that it's a \", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Regression API reference\n",
    "\n",
    "# * NOTE: Ascends-toolkit was developed to be used via command-line interface or web-based interface; however, if needed,\n",
    "# users may use ascends-toolkit's API. The following shows an example of performing a regression task using \n",
    "# the core ascends-toolkit APIs\n",
    "\n",
    "csv_file = 'data/BostonHousing.csv'\n",
    "cols_to_remove = []\n",
    "target_col = 'medv'\n",
    "\n",
    "# Load data from csv file\n",
    "# A standard csv file can be loaded and shuffled as follows\n",
    "\n",
    "data_df, x_train, y_train, header_x, header_y = asc.data_load_shuffle(csv_file, None, cols_to_remove, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* correlation_analysis_all\n",
      "Top-k features for each criteria\n",
      "{'PCC': ['rm', 'zn', 'b', 'dis', 'chas', 'age', 'rad', 'crim', 'nox', 'tax'], 'PCC_SQRT': ['lstat', 'rm', 'ptratio', 'indus', 'tax', 'nox', 'crim', 'rad', 'age', 'zn'], 'MIC': ['lstat', 'rm', 'nox', 'age', 'indus', 'ptratio', 'crim', 'tax', 'dis', 'zn'], 'MAS': ['chas', 'b', 'age', 'zn', 'rad', 'dis', 'nox', 'ptratio', 'crim', 'tax'], 'MEV': ['lstat', 'rm', 'nox', 'age', 'indus', 'ptratio', 'crim', 'tax', 'dis', 'zn'], 'MCN': ['zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio'], 'MCN_general': ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax'], 'GMIC': ['lstat', 'rm', 'nox', 'age', 'indus', 'ptratio', 'crim', 'tax', 'dis', 'rad'], 'TIC': ['lstat', 'rm', 'nox', 'indus', 'ptratio', 'age', 'crim', 'tax', 'dis', 'rad']}\n",
      "\n",
      "Full Correlation Analysis report\n",
      "              MIC       MAS       MEV       MCN  MCN_general      GMIC  \\\n",
      "age      0.420689  0.099268  0.420689  5.321928          2.0  0.368688   \n",
      "b        0.272469  0.112505  0.272469  5.321928          2.0  0.207560   \n",
      "chas     0.133026  0.113481  0.133026  5.321928          2.0  0.079504   \n",
      "crim     0.358757  0.044427  0.358757  5.000000          2.0  0.326953   \n",
      "dis      0.315033  0.055968  0.315033  5.321928          2.0  0.282479   \n",
      "indus    0.414140  0.039397  0.414140  5.321928          2.0  0.350791   \n",
      "lstat    0.615427  0.034828  0.615427  5.321928          2.0  0.563114   \n",
      "nox      0.442723  0.047576  0.442723  5.321928          2.0  0.390978   \n",
      "ptratio  0.371581  0.045671  0.371581  5.321928          2.0  0.335871   \n",
      "rad      0.278780  0.060237  0.278780  5.321928          2.0  0.238301   \n",
      "rm       0.450967  0.038707  0.450967  5.321928          2.0  0.429243   \n",
      "tax      0.324490  0.041496  0.324490  5.321928          2.0  0.287834   \n",
      "zn       0.289734  0.098851  0.289734  5.321928          2.0  0.236065   \n",
      "\n",
      "               TIC  PCC_SQRT       PCC  \n",
      "age      21.160602  0.376955 -0.376955  \n",
      "b        11.615744  0.333461  0.333461  \n",
      "chas      4.074539  0.175260  0.175260  \n",
      "crim     21.033617  0.388305 -0.388305  \n",
      "dis      18.077608  0.249929  0.249929  \n",
      "indus    23.199544  0.483725 -0.483725  \n",
      "lstat    38.803088  0.737663 -0.737663  \n",
      "nox      25.119383  0.427321 -0.427321  \n",
      "ptratio  21.688436  0.507787 -0.507787  \n",
      "rad      14.967824  0.381626 -0.381626  \n",
      "rm       28.144318  0.695360  0.695360  \n",
      "tax      19.708624  0.468536 -0.468536  \n",
      "zn       12.802512  0.360445  0.360445  \n"
     ]
    }
   ],
   "source": [
    "# Performing correlation analysis\n",
    "# Correlation analysis can be performed as follows\n",
    "# fs_dict will only contain the top-k features for each criteria (e.g., PCC)\n",
    "# final_report will contain the full evaluation scores for each feature\n",
    "\n",
    "k = 10\n",
    "fs_dict, final_report = asc.correlation_analysis_all(data_df, target_col, top_k = k, file_to_save = None, save_chart = None)\n",
    "\n",
    "print(\"Top-k features for each criteria\")\n",
    "print(fs_dict)\n",
    "print(\"\")\n",
    "print(\"Full Correlation Analysis report\")\n",
    "print(final_report)\n",
    "\n",
    "# To use top-k (k=10) features based on PCC (Pearson's correlation coefficient)\n",
    "\n",
    "input_col = fs_dict['PCC']\n",
    "\n",
    "# We need to load the file again\n",
    "data_df, x_train, y_train, header_x, header_y = asc.data_load_shuffle(csv_file, input_col, cols_to_remove, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE =  2.793895239226903 , R2 =  0.6994175646013248\n"
     ]
    }
   ],
   "source": [
    "# Generating a default model parameters\n",
    "model_parameters = asc.default_model_parameters() \n",
    "\n",
    "# Model Training\n",
    "model_type = 'RF' # model type can be 'LR','RF','NN','KR','BR','SVM'\n",
    "scaler_option = 'StandardScaler' # scaler option can be 'False','StandardScaler','Normalizer','MinMaxScaler','RobustScaler'\n",
    "num_of_folds = 5\n",
    "model = asc.define_model_regression(model_type, model_parameters, x_header_size = x_train.shape[1])\n",
    "predictions, actual_values = asc.train_and_predict(model, x_train, y_train, scaler_option=scaler_option, num_of_folds=num_of_folds)\n",
    "MAE, R2 = asc.evaluate(predictions, actual_values)\n",
    "\n",
    "# Printing the performance of regression task\n",
    "print(\"MAE = \", MAE,\", R2 = \", R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:  7.7min finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# tuning hyper parameters\n",
    "tuned_parameters = asc.hyperparameter_tuning(model_type, x_train, y_train\n",
    "                                             , num_of_folds, scaler_option\n",
    "                                           , n_iter=1000, random_state=0, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE =  2.8167252676854493 , R2 =  0.7039443187462722\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "model_type = 'RF' # model type can be 'LR','RF','NN','KR','BR','SVM'\n",
    "scaler_option = 'StandardScaler' # scaler option can be 'False','StandardScaler','Normalizer','MinMaxScaler','RobustScaler'\n",
    "num_of_folds = 5\n",
    "model = asc.define_model_regression(model_type, model_parameters, x_header_size = x_train.shape[1])\n",
    "predictions, actual_values = asc.train_and_predict(model, x_train, y_train, scaler_option=scaler_option, num_of_folds=num_of_folds)\n",
    "MAE, R2 = asc.evaluate(predictions, actual_values)\n",
    "\n",
    "# Printing the performance of regression task\n",
    "print(\"MAE = \", MAE,\", R2 = \", R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction-actual result in a csv file\n",
    "asc.save_test_data(predictions, actual_values, \"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training initiated ..\n",
      "* Training done.\n",
      "* Trained model saved to file: trained_model\n"
     ]
    }
   ],
   "source": [
    "# saving the trained model in a file\n",
    "asc.train_and_save(model, \"trained_model\", model_type\n",
    "                        , input_cols=header_x, target_col=header_y\n",
    "                        , x_train=x_train, y_train=y_train, scaler_option=scaler_option, path_to_save = '.', MAE=MAE, R2=R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model file loading and making a prediction can be done in the same way as the classification example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
